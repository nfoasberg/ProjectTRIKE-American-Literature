Working with MLA Data -- Second Steps

Previously on Project TRIKE: American Literature:

After downloading three years worth of metadata for the journal American Literature, we extracted the subject terms from it using Python.  The dataset included several types of subject terms specifying the centuries, authors, works, and types of works, in addition to what we're calling "subject terms" proper -- the descriptions of the content of each of the articles. Each of these types of terms is now stored in its own CSV file, along with the year from which it was taken. 

Our next step is to organize this data into tables and charts, so that we can more easily find interesting patterns.

From this point on, it's worthwhile to think about each of the pieces of our data separately; the things we can do with each are going to be different. 

American Literature by Century

Our collection of subject dates is the easiest to work with. In the records, each article has metadata indicating the century to which the literature it analyzes belongs -- the subject dates.  These are relatively easy to analyze because:

* They include a relatively small number of values
* The dates are comparable to each other in an obvious, straightfoward fashion.
* It's easy to know what questions we want to ask.

I'm making the data available here as a CSV file, but it's probably easiest to use some sort of spreadsheet software to tabulate the data and create simple visualizations. I used Microsoft Excel for this purpose, but you could do the same with GoogleSheets or a free and open source spreadsheet alternative.  

The software I used has a function allowing me to count the number of occurences for a specific string in a set of cels, so I organized the cels by publication year of the article (1950, 1980, or 2010) and made a table for each subject period and one for the data overall.  

subject_date_tables.csv

I used the visualization tools in my spreadsheet software to create some bar charts illustrating the coverage of literature by century. Now, the data is boiled down to a point where we can begin to make inferences about the focus of this journal! 

[insert one or more of the graphs from subject_date_visualizations.xml]

American Literature by Type of Work

The "work classification" -- dividing works into fiction, poetry, etc is a little more complicated than the dates, but not as complicated as some of the other metadata we'll consider later. 

This existence of a "genre" field which is apparently close in meaning to the "work classification" field complicates this. Should I combine these two fields or not?  The values of this field are often similar, and in all cases but one, "genre" appears to replace "work classification." However, there is certainly a difference behind the scenes -- otherwise, this wouldn't be two different fields. This difference might be a hierarchical one -- that is, it might exist at a different level of specificity in the vocabulary of the index. It's risky to combine fields when I don't fully understand why they are separate, but I also want to get the most accurate understanding that I can of the subject matter of the journal. Ultimately, I did combine them. Thus, the "Type of Work" data doesn't perfectly mirror the records; it's an amalgamated category. I'm already creating my own fields. 

Once again, I made a table of the different types of works appearing in the metadata, checking for duplicates as descibed above: 

work_classification_table.csv

Type of Work	Cited in 1950	Cited in 1980	Cited in 2010	Total
autobiography	0	0	2	2
criticism	1	0	0	1
drama	0	1	0	1
fiction	0	0	2	2
folk literature	0	1	0	1
historical novel	0	0	1	1
letters	0	1	0	1
manuscript note	2	0	0	2
no classification	17	2	1	20
novel	6	14	6	26
periodicals	2	0	0	2
poetic cycle	0	0	1	1
poetry	4	8	5	17
popular poetry	0	0	1	1
prose	9	5	6	20
short story	1	6	2	9
song	0	0	1	1
source study	1	0	0	1
translation	1	0	0	1
war fiction	0	0	1	1
Total	44	38	29	111

Looking at this table, a few problems become obvious. First, this is a much larger number of possible categories than the last dataset! Patterns are difficult to discern from this table; even  a bar chart like the one we made above wouldn't be particularly helpful.  Additionally, some of the distinctions made seem a little arbitrary -- for instance, why are some works classified as "novels" or "short stories," while others are classified simply as "prose"?

[insert "Types of Work by Year Cited" from work_classification_visualization.xls]

A quick glance shows us that most of these categories include fewer than five items.  Records without categories and items in three categories, "novel," "poetry," "prose," and "short stories," make up most of the list. One way of dealing with this kind of data is to look only at the items that dominate the list, so for instance, I could visualize just these three:

[insert "Most Prevalent Work Types" graph from work_classification_visualization.xls]

But this doesn't seem right to me. Why are we giving so much attention to these different types of prose? What if the categories I've discarded actually add up to something important? Are we underestimating the amount of criticism published about prose if we leave out the autobiographies and other prose genres?  

My answer to this problem was to sort each type of work into a larger category. The typical "genres" that literary criticism often uses as starting points are poetry, prose, and drama, so that seemed like a good place to start. However, I also wanted to acknowledge the difference between novels and manuscript notes, works of criticism, and the like. Therefore, I introduced into my categories a distinction between literary prose works, like novels and short stories, and critical prose works, such as criticism and manuscript notes. For less specific categories, I made some assumptions -- for instance, I assumed that "prose" referred to literary prose, and that the translation was a prose translation. These aren't necessarily true. In this case I could have gone back to my original downloaded records and painstakingly checked each one -- but then, what is the point of extracting the data in the first place? 

Most of the categories were fairly easy to group, but "songs" was a difficult one. I grouped it with poetry, because surely it is a type of verse -- but that classification is far from an obvious one. 

There is one more problem with this kind of grouping. When I initially compiled the dataset, I eliminated duplicate data from within the same record. However, by collapsing the categories into groups, it's possible that I could undo some of that work. For instance, if "manuscript notes" and "source study" both occurred in the same record, I am now counting it twice. That could be acceptable if I'm interested in 

work_classification_grouped.csv

I was not sure that my new groupings would reveal anything about the data, but they did! The new table looked like this:

work_classification_table.csv

Type of Work	Cited 1950	Cited 1980	Cited 2010	Total
Creative Prose Works	16	16	14	46
Critical Prose Works	5	5	0	10
Works of Poetry	4	4	4	12
Total	25	25	18	68

Or, when visualized, a little more like this:

[insert chart from work_classification_grouped_visualization.xls]

At first, I was suspicious of these results! The numbers were so consistent that I feared I had made an error in counting them up. However, checking my work confirmed that these numbers are accurate.  In fact, their consistency is so great that it suggests we have discovered one of the editorial policies of _American Literature_. 

This also makes it much easier to notice the absence of any articles focusing on drama!  

American Literature by Authors

A reminder: in this section, I'm looking at the authors who are the subject of the articles in American Literature, not the authors who wrote the articles. 

This is a much longer list than the ones we've worked with so far! To figure out exactly how much, I made a table that consolidated my list of authors by year cited. Sixty-two different subject authors were included in the metadata for these three years; only 11 were mentioned more than once.  The only two mentioned more than three times were Nathaniel Hawthorne (4 times) and Herman Melville (12 times). 

subject_authors_by_year.csv

This isn't very useful, because we don't have a large enough sample to draw conclusions about the extent to which American Literature features certain authors in their articles. Melville is clearly important, but the much larger group of authors analyzed in at least one article includes authors like Emily Dickinson and Robert Frost along with much more obscure authors like Bret Harte and primarily non-literary authors like Thomas Jefferson. With a broader look at the work published in American Literature, we would likely see much more work on Dickinson than on Harte. There are so many authors in play that it isn't especially surprising for any one author, even an important one to be only cited once in a year, or not cited at all.  

The subject authors gave me an opportunity to partially answer one of my research questions: How had the canon of American literature changed in the twentieth century? Are scholars writing about different authors now than they did previously? Is more attention being paid to women authors and authors of color? 

To answer that question, I needed to be able to identify authors by gender and race. This is problematic for several reasons! 

First, it requires me to slot people into reductive, simplistic categories, which may or may not match their own sense of their identity.  I've classified Willa Cather as a woman here; I'm not completely certain that this is how she identified, and since I'm not a Cather scholar, I'm aware that there are questions about this, but I'm ill-suited to make a strong argument for any particular gender. This is one problem with projects that cover multiple authors; it's very likely to come across researchers with whom one isn't familiar.  Race is also complex. Because I'm working with a small dataset, I looked up each author in an attempt to find the most appropriate way to identify them. Note that the "Group" category I left out of the analysis might have been helpful here, since it often identified authors in this way (for instance, as "African American woman poets").  However, it was also inconsistently applied. In fact, because the white supremacy ingrained in American culture views whiteness as the unmarked default, it is much easier to find out whether an author is of color than it is to confirm that an author is white, both in MLA and elsewhere. I tried not to make too many assumptions, but it's entirely possible that I'm incorrect in some places. Two of the authors in this sample were also biracial; in this case, they were Native and white, and I counted them as Native because I wanted to see to what extent the journal was publishing criticism on Native American authors. (Is there a better way to do this??)

In addition to the problems of who might fit in which categories for the purpose of this analysis, there are also some problems with using these categories in an apparently straightforward, unquestioning way. Miriam Posner argues convincingly that identity is more complex than crude data models such as those used in the US Census, and that the Census's simplifications don't give us the most accurate picture of the United States. When we don't acknowledge complexity, we run the risk of reinforcing these simplified and "official" categories. *

However, I do need to include information on both gender and race in order to identify underreprensentation.  It's worth thinking about better and more nuanced ways I could have accomplished this. For the time being, I came up with this:

subject_authors_coded.csv

From here, I made some tables breaking down the numbers of articles about works by the gender and race of the author.  This analysis does not take into account that some authors are the subject of multiple articles!  Thus, it does not evaluate (for instance) the number of women whose work was analyzed -- only the number of articles about literature by women.  Collapsing the works about the same author would produce different (though likely also interesting) results.

subject_authors_race_gender_table.csv

---from this point on, I haven't had time to revise --

American Literature by Works

When I began, I thought this would be an important category of metadata. However, we run into the same problems we do with many of the other categories: this is a list of sixty-nine items, with sixty-six different entries! For the curious, the works analyzed more than once are: Mark Twain's _Roughing It_ (mentioned in 1950 and 1980), _The Confidence Man_(mentioned in 1950 and 1980), and _The Grandissimes_(mentioned in 1980 and 2010).

If we were to break the works into groups, we might get interesting results -- but much of this work is already done in the rest of the metadata! We already have data about types of work, centuries published, and the authors. 

Thus, while I made a list of works, I've done no other analysis on this material.

subject_works_list_compiled.csv

## American Literature by Subject

The above data tells us a lot about the publication practices of _American Literature_, but now we are coming to the subject terms describing the articles, which are assigned by MLA field indexers.  The subjects tell us a lot about the journal and its content, but also about the bibliography and which aspects of an article are considered notable.

subject_terms_list_compiled.csv

The first thing we can do with this list is calculate the number of terms assigned in each year.  In 1950, a total of 44 subject terms were assigned to articles in _American Literature_. In 1980, that number rose to 95, and in 2010, 112. However, fewer articles were published in 2010 than in those other years!  Pulling from the data provided above, I as able to calculate the average number of terms per article:

	1950	1980	2010
Number of Subject Terms	44	95	112
Number of Articles Published	37	35	24
Subject Terms per Article	1.189189189	2.714285714	4.666666667

This is worth  keeping in mind during our analysis!  Since articles published in 2010 have nearly four times as many subject terms on average as those published in 1950, it won't necessarily be meaningful to show that a particular term or set of terms were used more often in 2010.  

However, aside from the sheer number of terms, this list of subject terms doesn't offer any obvious, immediate path to analytical conclusions -- much like many of the other datasets we've looked at here. I made a decision to code the subject terms, breaking them into groups for analysis. Under ideal circumstances, I could cross-check these with another person, but the constraints of the current project prevented that approach.

Note that this is very different from the kinds of coding we've done on previous iterations of this data, because here, I am making up my own categories. Thus, my analysis will depend very heavily on what kinds of categories I assign and how I understand those categories.  Note that it isn't always necessary to make up your own terms. Often, vocabularies have already been developed and documented by other researchers; other times, it may be possible to access the categories into which the terms were originally understood.*

Nobody should trust my analysis without taking a good look at the categories I developed!  This is where a data dictionary comes in handy. It helps the future users of my data to understand the decisions I made, and, as an added benefit, writing it helped me to develop my thinking further, and eliminate the categories I couldn't explain well. 

subject_terms_data_dictionary.txt
subject_terms_coded.csv

Of course, these categories are deeply rooted in my own point of view; there is no reason to think someone else looking at this data would come up with the same analysis. However, it's worth noticing that some of them, like those I've labeled "philosophical concepts" or "rhetorical techniques" are about the aspects of the work being analyzed, while others are about the author, or relationships _among_ works, or even the article itself. 

Visualizing any set of data with this many categories is likely to be difficult. In this case, a year-by-year graph can give us some indication of where it would be most useful to look, because it allows us a quick glance into how the number of headings of each type has changed relative to the others of its year. 

[chart from subject_terms_analysis.xml]

*Subject terms, in many cases, are part of a hierarchical vocabulary with their own implied context. However, these are not always visible or legible to the enduser, in this case me.